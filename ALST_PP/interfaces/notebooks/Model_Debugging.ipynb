{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2055a1dd",
   "metadata": {},
   "source": [
    "# Alstom Stock Price Prediction - Model Debugging and Improvement\n",
    "\n",
    "This notebook provides tools for debugging the prediction model, visualizing its performance, and making improvements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25196c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add parent directory to path for imports\n",
    "module_path = str(Path().absolute().parent.parent)\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "# Update the import to use the correct module path\n",
    "from interfaces.cli.stock_predictor_legacy import (\n",
    "    run, _build_features, _log_forward_return, _build_ensemble_classifier,\n",
    "    _build_stacked_classifier, _adaptive_threshold, _signals_from_proba\n",
    ")\n",
    "\n",
    "import yfinance as yf\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix, classification_report, roc_curve, precision_recall_curve, \n",
    "    auc, mean_squared_error, mean_absolute_error\n",
    ")\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.ensemble import RandomForestClassifier, HistGradientBoostingClassifier\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "# Configure plots\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = [12, 8]\n",
    "plt.rcParams['figure.dpi'] = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "957e54d8",
   "metadata": {},
   "source": [
    "## 1. Load and Examine Data\n",
    "\n",
    "First, let's load the data and examine the dataset structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5554c897",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "ticker = \"ALO.PA\"\n",
    "years = 5.0\n",
    "horizon = 5\n",
    "random_state = 42\n",
    "\n",
    "# Load data\n",
    "from datetime import datetime, timedelta\n",
    "end_date = datetime.today().date()\n",
    "start_date = end_date - timedelta(days=int(years * 365))\n",
    "\n",
    "print(f\"Downloading data for {ticker} from {start_date} to {end_date}\")\n",
    "data = yf.download(ticker, start=start_date, end=end_date, auto_adjust=True, progress=False)\n",
    "data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71502fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process data and build features\n",
    "data, features = _build_features(data, start_date, end_date)\n",
    "print(f\"Total features: {len(features)}\")\n",
    "print(f\"First 10 features: {features[:10]}\")\n",
    "\n",
    "# Display the first few rows of the processed data\n",
    "data.tail().T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a672cf13",
   "metadata": {},
   "source": [
    "## 2. Feature Analysis and Importance\n",
    "\n",
    "Let's analyze the features and their importance for predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eaa1e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target variable\n",
    "y_ret = _log_forward_return(data['AdjClose'], horizon).dropna()\n",
    "X = data.loc[y_ret.index, features]\n",
    "\n",
    "# Create a binary target (direction prediction)\n",
    "y_direction = (y_ret > 0).astype(int)\n",
    "\n",
    "print(f\"Total samples: {len(X)}\")\n",
    "print(f\"Positive class ratio: {y_direction.mean():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9f9c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation with target\n",
    "correlation_with_target = pd.DataFrame({'feature': features, \n",
    "                                         'correlation_with_return': [X[f].corr(y_ret) for f in features],\n",
    "                                         'correlation_with_direction': [X[f].corr(y_direction) for f in features]})\n",
    "\n",
    "# Sort by absolute correlation with direction\n",
    "correlation_with_target['abs_corr_direction'] = correlation_with_target['correlation_with_direction'].abs()\n",
    "correlation_sorted = correlation_with_target.sort_values('abs_corr_direction', ascending=False)\n",
    "\n",
    "# Plot top 20 correlations\n",
    "plt.figure(figsize=(12, 10))\n",
    "top_features = correlation_sorted.head(20)\n",
    "sns.barplot(x='correlation_with_direction', y='feature', data=top_features)\n",
    "plt.title('Top 20 Features by Correlation with Price Direction', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Display the correlation table\n",
    "correlation_sorted.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c326c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance using Random Forest\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "rf = RandomForestRegressor(n_estimators=200, random_state=random_state)\n",
    "rf.fit(X, y_ret)\n",
    "\n",
    "# Get feature importances\n",
    "feature_importance = pd.DataFrame({'feature': features, 'importance': rf.feature_importances_})\n",
    "feature_importance = feature_importance.sort_values('importance', ascending=False)\n",
    "\n",
    "# Plot top 20 features\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.barplot(x='importance', y='feature', data=feature_importance.head(20))\n",
    "plt.title('Top 20 Features by Random Forest Importance', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Display the importance table\n",
    "feature_importance.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e29541e",
   "metadata": {},
   "source": [
    "## 3. Model Performance Analysis\n",
    "\n",
    "Let's evaluate the model performance with detailed metrics and visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7047e1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use top features for model training\n",
    "top_n = 30\n",
    "top_features = feature_importance['feature'].head(top_n).tolist()\n",
    "X_reduced = X[top_features]\n",
    "\n",
    "# Time series cross-validation setup\n",
    "n_splits = max(3, min(8, len(X) // 80))\n",
    "tscv = TimeSeriesSplit(n_splits=n_splits, gap=5)\n",
    "\n",
    "# Collect results\n",
    "cv_results = {\n",
    "    'HistGradientBoosting': {'proba': [], 'true': [], 'indices': []},\n",
    "    'RandomForest': {'proba': [], 'true': [], 'indices': []},\n",
    "    'Ensemble': {'proba': [], 'true': [], 'indices': []}\n",
    "}\n",
    "\n",
    "# Run cross-validation for different models\n",
    "for model_name in cv_results.keys():\n",
    "    print(f\"\\nEvaluating {model_name}...\")\n",
    "    \n",
    "    all_idx = []\n",
    "    all_proba = []\n",
    "    all_true = []\n",
    "    \n",
    "    for train_idx, test_idx in tscv.split(X_reduced):\n",
    "        X_train, X_test = X_reduced.iloc[train_idx], X_reduced.iloc[test_idx]\n",
    "        y_train, y_test = y_direction.iloc[train_idx], y_direction.iloc[test_idx]\n",
    "        \n",
    "        # Select model\n",
    "        if model_name == 'HistGradientBoosting':\n",
    "            model = HistGradientBoostingClassifier(max_depth=5, learning_rate=0.05, max_iter=300, random_state=random_state)\n",
    "        elif model_name == 'RandomForest':\n",
    "            model = RandomForestClassifier(n_estimators=500, random_state=random_state)\n",
    "        else:  # Ensemble\n",
    "            model = _build_ensemble_classifier(random_state=random_state)\n",
    "            \n",
    "        # Calibrate\n",
    "        calibrated = CalibratedClassifierCV(model, cv=3, method='isotonic')\n",
    "        calibrated.fit(X_train, y_train)\n",
    "        \n",
    "        # Predictions\n",
    "        proba = calibrated.predict_proba(X_test)[:, 1]\n",
    "        \n",
    "        all_idx.append(test_idx)\n",
    "        all_proba.append(proba)\n",
    "        all_true.append(y_test.values)\n",
    "    \n",
    "    # Combine results\n",
    "    cv_results[model_name]['indices'] = np.concatenate(all_idx)\n",
    "    cv_results[model_name]['proba'] = np.concatenate(all_proba)\n",
    "    cv_results[model_name]['true'] = np.concatenate(all_true)\n",
    "    \n",
    "    # Performance metrics\n",
    "    order = np.argsort(cv_results[model_name]['indices'])\n",
    "    proba_sorted = cv_results[model_name]['proba'][order]\n",
    "    true_sorted = cv_results[model_name]['true'][order]\n",
    "    \n",
    "    # Calculate metrics\n",
    "    fpr, tpr, _ = roc_curve(true_sorted, proba_sorted)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    # Get predictions with threshold 0.5\n",
    "    pred = (proba_sorted >= 0.5).astype(int)\n",
    "    accuracy = np.mean(pred == true_sorted)\n",
    "    cm = confusion_matrix(true_sorted, pred)\n",
    "    \n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"ROC AUC: {roc_auc:.4f}\")\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67af6160",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ROC curves\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "for model_name, results in cv_results.items():\n",
    "    order = np.argsort(results['indices'])\n",
    "    proba_sorted = results['proba'][order]\n",
    "    true_sorted = results['true'][order]\n",
    "    \n",
    "    fpr, tpr, _ = roc_curve(true_sorted, proba_sorted)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    plt.plot(fpr, tpr, lw=2, label=f'{model_name} (AUC = {roc_auc:.3f})')\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate', fontsize=14)\n",
    "plt.ylabel('True Positive Rate', fontsize=14)\n",
    "plt.title('ROC Curves for Different Models', fontsize=16)\n",
    "plt.legend(loc=\"lower right\", fontsize=12)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dbbd6e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot calibration curves\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "for model_name, results in cv_results.items():\n",
    "    order = np.argsort(results['indices'])\n",
    "    proba_sorted = results['proba'][order]\n",
    "    true_sorted = results['true'][order]\n",
    "    \n",
    "    # Create bins for calibration curve\n",
    "    bins = np.linspace(0, 1, 11)\n",
    "    bin_centers = bins[:-1] + np.diff(bins) / 2\n",
    "    bin_indices = np.digitize(proba_sorted, bins) - 1\n",
    "    bin_indices = np.clip(bin_indices, 0, len(bin_centers) - 1)\n",
    "    \n",
    "    bin_sums = np.bincount(bin_indices, weights=true_sorted, minlength=len(bin_centers))\n",
    "    bin_counts = np.bincount(bin_indices, minlength=len(bin_centers))\n",
    "    bin_true_probs = np.zeros_like(bin_centers, dtype=float)\n",
    "    nonzero = bin_counts > 0\n",
    "    bin_true_probs[nonzero] = bin_sums[nonzero] / bin_counts[nonzero]\n",
    "    \n",
    "    plt.plot(bin_centers, bin_true_probs, marker='o', linestyle='-', lw=2, label=model_name)\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=2, label='Perfectly calibrated')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.0])\n",
    "plt.xlabel('Predicted probability', fontsize=14)\n",
    "plt.ylabel('True probability in each bin', fontsize=14)\n",
    "plt.title('Calibration Curves for Different Models', fontsize=16)\n",
    "plt.legend(loc=\"lower right\", fontsize=12)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71304552",
   "metadata": {},
   "source": [
    "## 4. Threshold Optimization\n",
    "\n",
    "Let's find the optimal threshold for our predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3e3341",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the ensemble model results for threshold optimization\n",
    "model_name = 'Ensemble'  # or choose another model\n",
    "results = cv_results[model_name]\n",
    "order = np.argsort(results['indices'])\n",
    "proba_sorted = results['proba'][order]\n",
    "true_sorted = results['true'][order]\n",
    "\n",
    "# Test different thresholds\n",
    "thresholds = np.linspace(0.3, 0.8, 51)\n",
    "metrics = {'threshold': [], 'accuracy': [], 'precision': [], 'recall': [], 'f1': []}\n",
    "\n",
    "for threshold in thresholds:\n",
    "    pred = (proba_sorted >= threshold).astype(int)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = np.mean(pred == true_sorted)\n",
    "    precision = np.sum((pred == 1) & (true_sorted == 1)) / max(1, np.sum(pred == 1))\n",
    "    recall = np.sum((pred == 1) & (true_sorted == 1)) / max(1, np.sum(true_sorted == 1))\n",
    "    f1 = 2 * precision * recall / max(1e-10, precision + recall)\n",
    "    \n",
    "    metrics['threshold'].append(threshold)\n",
    "    metrics['accuracy'].append(accuracy)\n",
    "    metrics['precision'].append(precision)\n",
    "    metrics['recall'].append(recall)\n",
    "    metrics['f1'].append(f1)\n",
    "\n",
    "# Convert to DataFrame\n",
    "metrics_df = pd.DataFrame(metrics)\n",
    "\n",
    "# Plot metrics vs threshold\n",
    "plt.figure(figsize=(12, 8))\n",
    "for metric in ['accuracy', 'precision', 'recall', 'f1']:\n",
    "    plt.plot(metrics_df['threshold'], metrics_df[metric], lw=2, label=metric.capitalize())\n",
    "\n",
    "plt.axhline(y=0.5, color='gray', linestyle='--', alpha=0.5, label='Baseline (0.5)')\n",
    "\n",
    "# Find best F1 threshold\n",
    "best_f1_idx = np.argmax(metrics_df['f1'])\n",
    "best_f1_threshold = metrics_df.loc[best_f1_idx, 'threshold']\n",
    "plt.axvline(x=best_f1_threshold, color='red', linestyle='--', \n",
    "           label=f'Best F1 threshold: {best_f1_threshold:.2f}')\n",
    "\n",
    "plt.xlabel('Threshold', fontsize=14)\n",
    "plt.ylabel('Score', fontsize=14)\n",
    "plt.title('Classification Metrics vs. Threshold', fontsize=16)\n",
    "plt.legend(loc=\"best\", fontsize=12)\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aefcd7a",
   "metadata": {},
   "source": [
    "## 5. Backtest with Optimal Parameters\n",
    "\n",
    "Now let's run a backtest with our optimized parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29daa76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best threshold from the analysis\n",
    "best_threshold = metrics_df.loc[best_f1_idx, 'threshold']\n",
    "print(f\"Best threshold from analysis: {best_threshold:.4f}\")\n",
    "\n",
    "# Run with optimized parameters\n",
    "result = run(\n",
    "    ticker=ticker,\n",
    "    years=years,\n",
    "    horizon=horizon,\n",
    "    threshold=best_threshold,\n",
    "    use_ensemble=True,  # Use the ensemble model\n",
    "    calibrate=True,\n",
    "    plot=True  # Generate a plot\n",
    ")\n",
    "\n",
    "# Display key results\n",
    "print(f\"\\nPrediction result for {ticker}:\")\n",
    "print(f\"Decision: {result['y_pred']}\")\n",
    "print(f\"Probability up: {result['proba_up']:.4f}\")\n",
    "print(f\"Adaptive threshold: {result['adaptive_threshold']:.4f}\")\n",
    "print(f\"\\nLast close: {result['last_close']:.4f}\")\n",
    "print(f\"Predicted return: {result['predicted_return']:.4f}\")\n",
    "print(f\"Predicted price: {result['predicted_price']:.4f}\")\n",
    "\n",
    "# Display backtest metrics\n",
    "backtest = result['metrics']['backtest']\n",
    "print(f\"\\nBacktest results:\")\n",
    "for key, value in backtest.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf934a8c",
   "metadata": {},
   "source": [
    "## 6. Error Analysis\n",
    "\n",
    "Let's analyze where the model is making mistakes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f72445a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the ensemble model results\n",
    "model_name = 'Ensemble'  # or choose another model\n",
    "results = cv_results[model_name]\n",
    "indices = results['indices']\n",
    "probas = results['proba']\n",
    "true_values = results['true']\n",
    "\n",
    "# Create DataFrame with predictions and actual values\n",
    "error_df = pd.DataFrame({\n",
    "    'date': X_reduced.index[indices],\n",
    "    'true_direction': true_values,\n",
    "    'predicted_proba': probas\n",
    "})\n",
    "\n",
    "# Add features to analyze\n",
    "for feature in top_features[:10]:  # Add top 10 features\n",
    "    error_df[feature] = X_reduced[feature].values[indices]\n",
    "\n",
    "# Add prediction with threshold\n",
    "error_df['predicted_direction'] = (error_df['predicted_proba'] >= best_threshold).astype(int)\n",
    "error_df['is_correct'] = error_df['predicted_direction'] == error_df['true_direction']\n",
    "error_df['error_type'] = np.where(error_df['is_correct'], 'Correct', \n",
    "                         np.where(error_df['predicted_direction'] == 1, 'False Positive', 'False Negative'))\n",
    "\n",
    "# Error rate by date\n",
    "error_df['year_month'] = pd.to_datetime(error_df['date']).dt.to_period('M')\n",
    "error_by_month = error_df.groupby('year_month').agg(\n",
    "    accuracy=('is_correct', 'mean'),\n",
    "    count=('is_correct', 'count')\n",
    ").reset_index()\n",
    "\n",
    "# Plot error rate over time\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.bar(error_by_month['year_month'].astype(str), error_by_month['accuracy'], alpha=0.7)\n",
    "plt.axhline(y=error_df['is_correct'].mean(), color='red', linestyle='--', \n",
    "           label=f'Overall accuracy: {error_df[\"is_correct\"].mean():.4f}')\n",
    "plt.xticks(rotation=90)\n",
    "plt.xlabel('Month', fontsize=14)\n",
    "plt.ylabel('Accuracy', fontsize=14)\n",
    "plt.title('Model Accuracy by Month', fontsize=16)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1763bc66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature distributions by error type\n",
    "feature = top_features[0]  # Change to analyze different features\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "for error_type in ['Correct', 'False Positive', 'False Negative']:\n",
    "    subset = error_df[error_df['error_type'] == error_type][feature]\n",
    "    sns.kdeplot(subset, label=f'{error_type} (n={len(subset)})')\n",
    "\n",
    "plt.xlabel(feature, fontsize=14)\n",
    "plt.ylabel('Density', fontsize=14)\n",
    "plt.title(f'Distribution of {feature} by Error Type', fontsize=16)\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ded18cb",
   "metadata": {},
   "source": [
    "## 7. Model Improvement Experiments\n",
    "\n",
    "Let's try some model improvements and compare them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c93ef8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate a model with specific parameters\n",
    "def evaluate_model_config(use_ensemble=True, calibrate=True, threshold=0.62, feature_count=30):\n",
    "    # Subset features if needed\n",
    "    if feature_count < len(features):\n",
    "        selected_features = feature_importance['feature'].head(feature_count).tolist()\n",
    "        X_subset = X[selected_features]\n",
    "    else:\n",
    "        X_subset = X\n",
    "    \n",
    "    # Time series cross-validation\n",
    "    n_splits = max(3, min(8, len(X) // 80))\n",
    "    tscv = TimeSeriesSplit(n_splits=n_splits, gap=5)\n",
    "    \n",
    "    all_idx, all_proba, all_true = [], [], []\n",
    "    \n",
    "    for train_idx, test_idx in tscv.split(X_subset):\n",
    "        X_train, X_test = X_subset.iloc[train_idx], X_subset.iloc[test_idx]\n",
    "        y_train, y_test = y_direction.iloc[train_idx], y_direction.iloc[test_idx]\n",
    "        \n",
    "        # Select model type\n",
    "        if use_ensemble:\n",
    "            model = _build_ensemble_classifier(random_state=random_state)\n",
    "        else:\n",
    "            model = HistGradientBoostingClassifier(max_depth=5, learning_rate=0.05, \n",
    "                                                  max_iter=300, random_state=random_state)\n",
    "        \n",
    "        # Apply calibration if needed\n",
    "        if calibrate:\n",
    "            model = CalibratedClassifierCV(model, cv=3, method='isotonic')\n",
    "            \n",
    "        model.fit(X_train, y_train)\n",
    "        proba = model.predict_proba(X_test)[:, 1]\n",
    "        \n",
    "        all_idx.append(test_idx)\n",
    "        all_proba.append(proba)\n",
    "        all_true.append(y_test.values)\n",
    "    \n",
    "    # Combine results\n",
    "    idx_oof = np.concatenate(all_idx)\n",
    "    proba_oof = np.concatenate(all_proba)\n",
    "    true_oof = np.concatenate(all_true)\n",
    "    \n",
    "    # Sort by index\n",
    "    order = np.argsort(idx_oof)\n",
    "    proba_sorted = proba_oof[order]\n",
    "    true_sorted = true_oof[order]\n",
    "    \n",
    "    # Calculate metrics\n",
    "    pred = (proba_sorted >= threshold).astype(int)\n",
    "    accuracy = np.mean(pred == true_sorted)\n",
    "    \n",
    "    precision = np.sum((pred == 1) & (true_sorted == 1)) / max(1, np.sum(pred == 1))\n",
    "    recall = np.sum((pred == 1) & (true_sorted == 1)) / max(1, np.sum(true_sorted == 1))\n",
    "    f1 = 2 * precision * recall / max(1e-10, precision + recall)\n",
    "    \n",
    "    fpr, tpr, _ = roc_curve(true_sorted, proba_sorted)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'roc_auc': roc_auc,\n",
    "        'config': {\n",
    "            'use_ensemble': use_ensemble,\n",
    "            'calibrate': calibrate,\n",
    "            'threshold': threshold,\n",
    "            'feature_count': feature_count\n",
    "        }\n",
    "    }\n",
    "\n",
    "# Try different configurations\n",
    "experiment_configs = [\n",
    "    {'use_ensemble': True, 'calibrate': True, 'threshold': 0.62, 'feature_count': 30},   # Baseline\n",
    "    {'use_ensemble': True, 'calibrate': True, 'threshold': best_threshold, 'feature_count': 30},  # Optimized threshold\n",
    "    {'use_ensemble': True, 'calibrate': True, 'threshold': 0.62, 'feature_count': 50},   # More features\n",
    "    {'use_ensemble': False, 'calibrate': True, 'threshold': 0.62, 'feature_count': 30},  # No ensemble\n",
    "    {'use_ensemble': True, 'calibrate': False, 'threshold': 0.62, 'feature_count': 30}   # No calibration\n",
    "]\n",
    "\n",
    "# Run experiments\n",
    "experiment_results = []\n",
    "for config in experiment_configs:\n",
    "    print(f\"Running experiment with config: {config}\")\n",
    "    result = evaluate_model_config(**config)\n",
    "    experiment_results.append(result)\n",
    "    print(f\"  Accuracy: {result['accuracy']:.4f}, F1: {result['f1']:.4f}, AUC: {result['roc_auc']:.4f}\")\n",
    "\n",
    "# Convert to DataFrame for comparison\n",
    "experiment_df = pd.DataFrame(experiment_results)\n",
    "experiment_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "084e75cc",
   "metadata": {},
   "source": [
    "## 8. Hyperparameter Tuning\n",
    "\n",
    "Let's tune the hyperparameters for the best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46392db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "# Hyperparameters to tune\n",
    "param_grid = {\n",
    "    'max_depth': [3, 5, 7, 9],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'max_iter': [200, 300, 500]\n",
    "}\n",
    "\n",
    "# Subset features based on best experiment\n",
    "best_experiment = experiment_df.loc[experiment_df['f1'].idxmax()]\n",
    "best_feature_count = best_experiment['config']['feature_count']\n",
    "selected_features = feature_importance['feature'].head(best_feature_count).tolist()\n",
    "X_subset = X[selected_features]\n",
    "\n",
    "# Time series CV\n",
    "n_splits = max(3, min(5, len(X) // 100))  # Fewer splits for speed\n",
    "tscv = TimeSeriesSplit(n_splits=n_splits, gap=5)\n",
    "\n",
    "# Results container\n",
    "tuning_results = []\n",
    "\n",
    "# Grid search through parameters\n",
    "for params in ParameterGrid(param_grid):\n",
    "    f1_scores = []\n",
    "    \n",
    "    for train_idx, test_idx in tscv.split(X_subset):\n",
    "        X_train, X_test = X_subset.iloc[train_idx], X_subset.iloc[test_idx]\n",
    "        y_train, y_test = y_direction.iloc[train_idx], y_direction.iloc[test_idx]\n",
    "        \n",
    "        # Create and train model\n",
    "        model = HistGradientBoostingClassifier(\n",
    "            max_depth=params['max_depth'],\n",
    "            learning_rate=params['learning_rate'],\n",
    "            max_iter=params['max_iter'],\n",
    "            random_state=random_state\n",
    "        )\n",
    "        \n",
    "        model.fit(X_train, y_train)\n",
    "        pred = model.predict(X_test)\n",
    "        \n",
    "        # Calculate F1 score\n",
    "        precision = np.sum((pred == 1) & (y_test.values == 1)) / max(1, np.sum(pred == 1))\n",
    "        recall = np.sum((pred == 1) & (y_test.values == 1)) / max(1, np.sum(y_test.values == 1))\n",
    "        f1 = 2 * precision * recall / max(1e-10, precision + recall)\n",
    "        f1_scores.append(f1)\n",
    "    \n",
    "    # Store average F1 score\n",
    "    tuning_results.append({\n",
    "        'params': params,\n",
    "        'mean_f1': np.mean(f1_scores)\n",
    "    })\n",
    "    print(f\"Params: {params}, Mean F1: {np.mean(f1_scores):.4f}\")\n",
    "\n",
    "# Find best parameters\n",
    "tuning_df = pd.DataFrame(tuning_results)\n",
    "best_params = tuning_df.loc[tuning_df['mean_f1'].idxmax()]['params']\n",
    "print(f\"\\nBest parameters: {best_params}\")\n",
    "print(f\"Best F1 score: {tuning_df['mean_f1'].max():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e387f9",
   "metadata": {},
   "source": [
    "## 9. Final Model with Optimized Parameters\n",
    "\n",
    "Let's run a prediction with our optimized model configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbcd636b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final optimized parameters\n",
    "final_params = {\n",
    "    'ticker': ticker,\n",
    "    'years': years,\n",
    "    'horizon': horizon,\n",
    "    'threshold': best_threshold,  # From threshold optimization\n",
    "    'use_ensemble': True,  # Based on experiment results\n",
    "    'calibrate': True,  # Based on experiment results\n",
    "    'plot': True\n",
    "}\n",
    "\n",
    "# Run prediction with optimized parameters\n",
    "optimized_result = run(**final_params)\n",
    "\n",
    "# Display key results\n",
    "print(f\"\\nOptimized prediction result for {ticker}:\")\n",
    "print(f\"Decision: {optimized_result['y_pred']}\")\n",
    "print(f\"Probability up: {optimized_result['proba_up']:.4f}\")\n",
    "print(f\"Adaptive threshold: {optimized_result['adaptive_threshold']:.4f}\")\n",
    "print(f\"\\nLast close: {optimized_result['last_close']:.4f}\")\n",
    "print(f\"Predicted return: {optimized_result['predicted_return']:.4f}\")\n",
    "print(f\"Predicted price: {optimized_result['predicted_price']:.4f}\")\n",
    "\n",
    "# Calculate expected return\n",
    "expected_return = optimized_result['predicted_price'] / optimized_result['last_close'] - 1\n",
    "print(f\"\\nExpected return: {expected_return:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e028d8",
   "metadata": {},
   "source": [
    "## 10. Recommendations for Model Improvement\n",
    "\n",
    "Based on our analysis, here are some recommendations for improving the model:\n",
    "\n",
    "1. **Optimal threshold:** Use the optimized threshold value of {best_threshold:.2f} instead of the default 0.62\n",
    "\n",
    "2. **Ensemble models:** The ensemble approach generally performs better than single models\n",
    "\n",
    "3. **Calibration:** Always use probability calibration for more reliable decision thresholds\n",
    "\n",
    "4. **Feature selection:** Focus on the top {best_feature_count} features for better signal-to-noise ratio\n",
    "\n",
    "5. **Model hyperparameters:** Use the optimized hyperparameters for HistGradientBoostingClassifier\n",
    "\n",
    "6. **Additional features to consider:**\n",
    "   - Sector performance indicators (transportation/railway sector ETFs)\n",
    "   - Broader macroeconomic indicators (interest rates, PMI)\n",
    "   - News sentiment analysis for Alstom\n",
    "   - Order book data if available\n",
    "\n",
    "7. **Volatility-based adjustments:** The adaptive thresholding seems to work well and could be further enhanced\n",
    "\n",
    "8. **Model ensembling:** Consider adding more diverse models to the ensemble (LightGBM, XGBoost)\n",
    "\n",
    "9. **Time-based features:** Add more calendar effects and cyclical features"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
